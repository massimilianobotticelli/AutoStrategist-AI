{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff628b60-cfec-422a-9c04-52ef268a54e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks_langchain\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8031fd-e9a4-47b7-bdcb-49130342eb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, ListOutputParser\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66785a4c-460d-45fb-a2b2-e9eb4a4b771e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22912761-5004-41de-a0f5-356bec68e80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = ChatDatabricks(endpoint=\"databricks-gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9685d4d-53c4-4a90-b601-8a51d626c622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt_template_content = \"\"\"\n",
    "You will get a free text. You need to extraxt the following information, if available:\n",
    "- manufacturer\n",
    "- model\n",
    "- year\n",
    "- price\n",
    "- odometer in km\n",
    "- transmission\n",
    "- fuel\n",
    "- drive (4wd, fwd, ...)\n",
    "- size (mid-size, full-size, ...)\n",
    "- type (SUV, hatchback, sedan)\n",
    "- paint_color\n",
    "- condition (like new, good, excellent, ...).\n",
    "\n",
    "If some fields are not found in the text, return them as null.\n",
    "\n",
    "Do not add any comment, answer only with a JSON format.\n",
    "\n",
    "EXAMPLE:\n",
    "\n",
    "free text: 2019 Ford Focus Sedan 2.0L 4dr Sedan 4WD 2019 Ford Focus Sedan 2.0L\n",
    "\n",
    "answer:\n",
    "{{\n",
    "    \"manufacturer\": \"ford\",\n",
    "    \"model\": \"focus\",\n",
    "    \"year\": \"2019\",\n",
    "    \"price\": null,\n",
    "    \"odometer\": null,\n",
    "    \"transmission\": null,\n",
    "    \"fuel\": null,\n",
    "    \"drive\": \"4wd\",\n",
    "    \"size\": null,\n",
    "    \"type\": \"sedan\",\n",
    "    \"paint_color\": null\n",
    "    }}\n",
    "\n",
    "free text: {free_text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template_content,\n",
    "    input_variables=[\"free_text\"]\n",
    "    )\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09e1739-cbcc-41f6-8365-c603d51cd4b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "# Filter out the Pydantic UserWarning because of ChatDatabricks and Reasoning model\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec868a2-68d1-4a96-b385-6515b64d2ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model\n",
    "\n",
    "def call_gpt_oss(message):\n",
    "    response = chain.invoke(message)\n",
    "    answer = json.loads(response.content)[-1]['text']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3b1c4e7-3050-490c-ae99-61e87d4eb011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_portion_updated = df_portion.toPandas()\n",
    "updates_list = []\n",
    "ulimit = 3 # df_portion_updated.shape[0]\n",
    "\n",
    "for i in range(0, ulimit):\n",
    "    message = {\"free_text\": df_portion_updated.iloc[i]['description']}\n",
    "    # print(message)\n",
    "    json_answer = call_gpt_oss(message)\n",
    "    json_info = json.loads(json_answer)\n",
    "\n",
    "    row_info = pd.Series(json_info)\n",
    "    row_info = row_info.dropna()\n",
    "    row_info[\"_original_index\"] = df_portion_updated.index[i]\n",
    "    updates_list.append(row_info)\n",
    "\n",
    "df_updates = pd.DataFrame(updates_list).set_index(\"_original_index\")\n",
    "df_portion_updated.update(df_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5edc68-05ea-4675-82c5-9a1139fe7055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_portion_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1567db9-cfa1-4447-9414-892c1e9f7392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0865faa-aa31-46e8-b24d-313fbfe0320a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab40d769-c358-41a4-835a-4848460c243e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define the structure of the JSON you expect from the LLM\n",
    "# Adjust types (Integer vs String) as needed based on your Pydantic model\n",
    "schema = StructType([\n",
    "    StructField(\"manufacturer\", StringType(), True),\n",
    "    StructField(\"model\", StringType(), True),\n",
    "    StructField(\"year\", StringType(), True),  # Keeping as string to be safe\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"odometer\", StringType(), True),\n",
    "    StructField(\"transmission\", StringType(), True),\n",
    "    StructField(\"fuel\", StringType(), True),\n",
    "    StructField(\"drive\", StringType(), True),\n",
    "    StructField(\"size\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"paint_color\", StringType(), True),\n",
    "    StructField(\"condition\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1422d9d7-b731-4918-8511-5e9f15b5edb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_portion.limit(3).write.mode(\"overwrite\").saveAsTable(\"workspace.car_sales.vehicles_tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "777916a6-e609-4857-ae76-cd264e90f46b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tiny = spark.table(\"workspace.car_sales.vehicles_tiny\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39035f38-56bf-4715-97f1-d40818a7b9c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "selected_ids = df_tiny.select(\"id\")\n",
    "\n",
    "df_selected = vehicles_df.where(\n",
    "    col(\"id\").isin(selected_ids)\n",
    ")\n",
    "display(df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea7933c-faf7-412a-ac22-94ad3eaf2c6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fcd655b-77b1-48e7-b40f-cb81ba7b236b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763812413541}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Iterator\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define the structure of the JSON you expect from the LLM\n",
    "# Adjust types (Integer vs String) as needed based on your Pydantic model\n",
    "schema = StructType([\n",
    "    StructField(\"manufacturer\", StringType(), True),\n",
    "    StructField(\"model\", StringType(), True),\n",
    "    StructField(\"year\", StringType(), True),  # Keeping as string to be safe\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"odometer\", StringType(), True),\n",
    "    StructField(\"transmission\", StringType(), True),\n",
    "    StructField(\"fuel\", StringType(), True),\n",
    "    StructField(\"drive\", StringType(), True),\n",
    "    StructField(\"size\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"paint_color\", StringType(), True),\n",
    "    StructField(\"condition\", StringType(), True)\n",
    "])\n",
    "\n",
    "# --- 1. CAPTURE CREDENTIALS (DRIVER SIDE) ---\n",
    "# We get these here to pass them into the closure of the UDF\n",
    "ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "db_host = ctx.apiUrl().get()\n",
    "db_token = ctx.apiToken().get()\n",
    "\n",
    "prompt_template_content = \"\"\"\n",
    "You will get a free text. You need to extraxt the following information, if available:\n",
    "- manufacturer\n",
    "- model\n",
    "- year\n",
    "- price\n",
    "- odometer in km\n",
    "- transmission\n",
    "- fuel\n",
    "- drive (4wd, fwd, ...)\n",
    "- size (mid-size, full-size, ...)\n",
    "- type (SUV, hatchback, sedan)\n",
    "- paint_color\n",
    "- condition (like new, good, excellent, ...).\n",
    "\n",
    "If some fields are not found in the text, return them as null.\n",
    "\n",
    "Do not add any comment, answer only with a JSON format.\n",
    "\n",
    "EXAMPLE:\n",
    "\n",
    "free text: 2019 Ford Focus Sedan 2.0L 4dr Sedan 4WD 2019 Ford Focus Sedan 2.0L\n",
    "\n",
    "answer:\n",
    "{{\n",
    "    \"manufacturer\": \"ford\",\n",
    "    \"model\": \"focus\",\n",
    "    \"year\": \"2019\",\n",
    "    \"price\": null,\n",
    "    \"odometer\": null,\n",
    "    \"transmission\": null,\n",
    "    \"fuel\": null,\n",
    "    \"drive\": \"4wd\",\n",
    "    \"size\": null,\n",
    "    \"type\": \"sedan\",\n",
    "    \"paint_color\": null\n",
    "    }}\n",
    "\n",
    "free text: {free_text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template_content,\n",
    "    input_variables=[\"free_text\"]\n",
    "    )\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# --- 2. DEFINE THE ITERATOR UDF ---\n",
    "@pandas_udf(schema)\n",
    "def extract_vehicle_info_udf(iterator: Iterator[pd.Series]) -> Iterator[pd.DataFrame]:\n",
    "    \n",
    "    # --- WORKER SETUP (Runs once per partition) ---\n",
    "    # Inject credentials so ChatDatabricks can authenticate\n",
    "    os.environ[\"DATABRICKS_HOST\"] = db_host\n",
    "    os.environ[\"DATABRICKS_TOKEN\"] = db_token\n",
    "    \n",
    "    # Initialize Model & Chain inside the worker\n",
    "    model = ChatDatabricks(endpoint=\"databricks-gpt-oss-120b\")\n",
    "    \n",
    "    chain = prompt | model \n",
    "\n",
    "    # Loop through batches (Partitions)\n",
    "    for descriptions in iterator:\n",
    "        results = []\n",
    "        \n",
    "        # Loop through rows in the batch\n",
    "        for text in descriptions:\n",
    "            try:\n",
    "                # 1. Invoke\n",
    "                response = chain.invoke({\"free_text\": text})\n",
    "                \n",
    "                # 2. Extract Content\n",
    "                # The 'content' might be a stringified list OR a python list depending on the model driver\n",
    "                raw_content = response.content\n",
    "\n",
    "                json_str = json.loads(response.content)[-1]['text']\n",
    "\n",
    "                # 3. JSON Load\n",
    "                data = json.loads(json_str)\n",
    "                results.append(data)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log the error in a way that doesn't crash the job\n",
    "                # You can inspect null rows later\n",
    "                results.append({})\n",
    "        \n",
    "        # Yield the batch\n",
    "        yield pd.DataFrame(results)\n",
    "\n",
    "# --- STEP 3: RUN INFERENCE ---\n",
    "# This creates the temporary 'extracted_data' struct column\n",
    "df_processed = df_tiny.withColumn(\"extracted_data\", extract_vehicle_info_udf(\"description\"))\n",
    "\n",
    "display(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a7ed20d-f238-470f-b64f-a82e6de65034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, coalesce\n",
    "\n",
    "# 1. Get the schema from the struct to know what we are working with\n",
    "extracted_schema = df_processed.schema[\"extracted_data\"].dataType\n",
    "\n",
    "# 2. Define the list of columns to backfill\n",
    "fillable_cols = extracted_schema.names \n",
    "\n",
    "# 3. Build a safe projection list\n",
    "final_columns = []\n",
    "\n",
    "for c in df_processed.columns:\n",
    "    # We skip the 'extracted_data' column itself\n",
    "    if c == \"extracted_data\":\n",
    "        continue\n",
    "        \n",
    "    if c in fillable_cols:\n",
    "        # --- ROBUST MERGE LOGIC ---\n",
    "        \n",
    "        # A. Get the Intended Type (String) from the UDF Schema\n",
    "        target_type = extracted_schema[c].dataType\n",
    "        \n",
    "        # B. Access the extracted field safely using Item/Bracket notation \n",
    "        #    (Fixes potential keyword conflicts with \"size\", \"type\", \"year\")\n",
    "        extracted_col = col(\"extracted_data\")[c]\n",
    "        \n",
    "        # C. Cast BOTH to the target type to ensure compatibility\n",
    "        #    (e.g. Cast Original Double 'size' -> String)\n",
    "        original_casted = col(c).cast(target_type)\n",
    "        extracted_casted = extracted_col.cast(target_type)\n",
    "        \n",
    "        # D. Coalesce (Take Original if exists, else Extracted)\n",
    "        final_columns.append(\n",
    "            coalesce(original_casted, extracted_casted).alias(c)\n",
    "        )\n",
    "    else:\n",
    "        # Keep unrelated columns (id, description, etc.)\n",
    "        final_columns.append(col(c))\n",
    "\n",
    "# 4. Apply and Debug\n",
    "df_final = df_processed.select(*final_columns)\n",
    "\n",
    "display(df_final)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_extract_details_dev",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
