{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff628b60-cfec-422a-9c04-52ef268a54e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks_langchain\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7c6705-0af1-4aae-a947-90bb624860f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = spark.table('workspace.car_sales.vehicles_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1d2c83-30b4-4c81-a939-d4b6e85f9534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf = df_cleaned.toPandas()\n",
    "initial_null_entries = pdf.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fcd655b-77b1-48e7-b40f-cb81ba7b236b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Iterator\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# LangChain / Databricks imports\n",
    "from databricks_langchain import ChatDatabricks\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION & SCHEMA\n",
    "# ==============================================================================\n",
    "\n",
    "# Define the schema for the output JSON. \n",
    "# Spark requires a strict schema for the UDF return type.\n",
    "# We use StringType for most fields to avoid casting errors during extraction.\n",
    "schema = StructType([\n",
    "    StructField(\"manufacturer\", StringType(), True),\n",
    "    StructField(\"model\", StringType(), True),\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"odometer\", StringType(), True),\n",
    "    StructField(\"transmission\", StringType(), True),\n",
    "    StructField(\"fuel\", StringType(), True),\n",
    "    StructField(\"drive\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"paint_color\", StringType(), True),\n",
    "    StructField(\"condition\", StringType(), True)\n",
    "])\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CREDENTIAL CAPTURE (DRIVER SIDE)\n",
    "# ==============================================================================\n",
    "\n",
    "# We capture the notebook's authentication context here on the Driver.\n",
    "# These variables will be \"closed over\" (pickled) and sent to the Worker nodes\n",
    "# where the UDF actually runs.\n",
    "ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "db_host = ctx.apiUrl().get()\n",
    "db_token = ctx.apiToken().get()\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PROMPT DEFINITION\n",
    "# ==============================================================================\n",
    "\n",
    "prompt_template_content = \"\"\"\n",
    "You will get a free text. You need to extract the following information, if available:\n",
    "- manufacturer\n",
    "- model\n",
    "- year\n",
    "- price\n",
    "- odometer in km\n",
    "- transmission\n",
    "- fuel\n",
    "- drive (4wd, fwd, ...)\n",
    "- type (SUV, hatchback, sedan)\n",
    "- paint_color\n",
    "- condition (like new, good, excellent, ...).\n",
    "\n",
    "If some fields are not found in the text, return them as null.\n",
    "Do not add any comment, answer only with a JSON format.\n",
    "\n",
    "EXAMPLE:\n",
    "free text: 2019 Ford Focus Sedan 2.0L 4dr Sedan 4WD 2019 Ford Focus Sedan 2.0L\n",
    "answer:\n",
    "{{\n",
    "    \"manufacturer\": \"ford\",\n",
    "    \"model\": \"focus\",\n",
    "    \"year\": \"2019\",\n",
    "    \"price\": null,\n",
    "    \"odometer\": null,\n",
    "    \"transmission\": null,\n",
    "    \"fuel\": null,\n",
    "    \"drive\": \"4wd\",\n",
    "    \"type\": \"sedan\",\n",
    "    \"paint_color\": null,\n",
    "    \"condition\": null\n",
    "}}\n",
    "\n",
    "free text: {free_text}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template_content,\n",
    "    input_variables=[\"free_text\"]\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. ITERATOR UDF DEFINITION\n",
    "# ==============================================================================\n",
    "\n",
    "@pandas_udf(schema)\n",
    "def extract_vehicle_info_udf(iterator: Iterator[pd.Series]) -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    A Scalar Iterator UDF that processes data in batches (partitions).\n",
    "    Using an Iterator allows us to initialize the LLM client ONCE per partition,\n",
    "    rather than once per row, which significantly improves performance.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- WORKER INITIALIZATION (Runs once per partition) ---\n",
    "    \n",
    "    # Inject credentials into the Worker's environment variables.\n",
    "    # The 'ChatDatabricks' client will look for these automatically.\n",
    "    os.environ[\"DATABRICKS_HOST\"] = db_host\n",
    "    os.environ[\"DATABRICKS_TOKEN\"] = db_token\n",
    "    \n",
    "    # Initialize the Model & Chain\n",
    "    # We use a \"Reasoning\" model (e.g., 120b) or standard model.\n",
    "    model = ChatDatabricks(endpoint=\"databricks-gpt-oss-120b\")\n",
    "    chain = prompt | model \n",
    "\n",
    "    # --- BATCH PROCESSING ---\n",
    "    for descriptions in iterator:\n",
    "        results = []\n",
    "        \n",
    "        # Iterate through rows in the current batch\n",
    "        for text in descriptions:\n",
    "            try:\n",
    "                # 1. Inference\n",
    "                response = chain.invoke({\"free_text\": text})\n",
    "                raw_content = response.content\n",
    "                \n",
    "                # 2. Robust Parsing Logic\n",
    "                # Handles three cases:\n",
    "                #   A. List Object (common in Reasoning models)\n",
    "                #   B. Stringified List (string representation of A)\n",
    "                #   C. Plain JSON String (standard models)\n",
    "                \n",
    "                final_json_str = \"{}\"\n",
    "                \n",
    "                # CASE A: The content is already a Python List\n",
    "                if isinstance(raw_content, list):\n",
    "                    text_block = next((item for item in raw_content if item.get('type') == 'text'), None)\n",
    "                    if text_block:\n",
    "                        final_json_str = text_block['text']\n",
    "                \n",
    "                # CASE B & C: The content is a String\n",
    "                elif isinstance(raw_content, str):\n",
    "                    cleaned = raw_content.strip()\n",
    "                    \n",
    "                    # Check for Stringified List (starts with [ and contains \"type\")\n",
    "                    if cleaned.startswith(\"[\") and \"type\" in cleaned:\n",
    "                        try:\n",
    "                            parsed_list = json.loads(cleaned)\n",
    "                            text_block = next((item for item in parsed_list if item.get('type') == 'text'), None)\n",
    "                            if text_block:\n",
    "                                final_json_str = text_block['text']\n",
    "                        except:\n",
    "                            pass # Parsing failed, fall through to default\n",
    "                    else:\n",
    "                        # Assume it's a direct JSON string, strip markdown if present\n",
    "                        final_json_str = cleaned.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "                # 3. Final JSON Load\n",
    "                # Ensure we have a valid JSON string before parsing\n",
    "                if not final_json_str: \n",
    "                    final_json_str = \"{}\"\n",
    "                    \n",
    "                data = json.loads(final_json_str)\n",
    "                results.append(data)\n",
    "\n",
    "            except Exception:\n",
    "                # Fail-safe: Return an empty dict to preserve row count.\n",
    "                # This results in a row of nulls in Spark, which is safer than crashing.\n",
    "                results.append({})\n",
    "        \n",
    "        # Yield the result batch as a Pandas DataFrame\n",
    "        yield pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ab863c-6851-4bc1-a15d-0207c6474c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RUN INFERENCE ---\n",
    "df_processed = df_cleaned.withColumn(\"extracted_data\", extract_vehicle_info_udf(\"description\")) # .repartition(8)\n",
    "\n",
    "display(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a7ed20d-f238-470f-b64f-a82e6de65034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, coalesce\n",
    "\n",
    "# ==============================================================================\n",
    "# SMART MERGE / BACKFILL LOGIC\n",
    "# ==============================================================================\n",
    "# This script merges the original columns with the LLM-extracted data.\n",
    "# Logic: Prioritize Original Data. If Original is Null, use Extracted Data.\n",
    "#\n",
    "# Solves two specific PySpark problems:\n",
    "# 1. Type Mismatch: Handles merging an empty Double column (Original) with a String (LLM).\n",
    "# 2. Keyword Conflicts: Handles columns named \"size\", \"type\", \"year\" without \n",
    "#    confusing Spark's SQL parser.\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. INSPECT SCHEMA\n",
    "# We dynamically retrieve the schema from the 'extracted_data' struct column.\n",
    "# This ensures we only try to merge fields that actually exist in the UDF output.\n",
    "extracted_schema = df_processed.schema[\"extracted_data\"].dataType\n",
    "\n",
    "# 2. DEFINE TARGET COLUMNS\n",
    "fillable_cols = extracted_schema.names \n",
    "\n",
    "# 3. BUILD PROJECTION LIST\n",
    "# We construct a list of column expressions to apply in a single .select() call.\n",
    "# This is more efficient than looping with .withColumn().\n",
    "final_columns = []\n",
    "\n",
    "for c in df_processed.columns:\n",
    "    # Skip the temporary 'extracted_data' struct itself (it will be dropped implicitly)\n",
    "    if c == \"extracted_data\":\n",
    "        continue\n",
    "        \n",
    "    if c in fillable_cols:\n",
    "        # --- ROBUST MERGE LOGIC ----------------------------------------------\n",
    "        \n",
    "        # A. Identify Target Type\n",
    "        #    Get the data type defined in the UDF schema (usually StringType).\n",
    "        target_type = extracted_schema[c].dataType\n",
    "        \n",
    "        # B. Access Extracted Field Safely\n",
    "        #    CRITICAL: We use bracket notation `col(\"struct\")[name]` instead of \n",
    "        #    dot notation `col(\"struct.name\")`. \n",
    "        #    This prevents Spark from confusing the column \"size\" with the \n",
    "        #    built-in SQL function SIZE().\n",
    "        extracted_col = col(\"extracted_data\")[c]\n",
    "        \n",
    "        # C. Harmonize Types (Double Cast)\n",
    "        #    If original 'size' is Double (because it was empty) and extracted \n",
    "        #    'size' is String (\"full-size\"), a direct coalesce would fail.\n",
    "        #    We cast the Original column to match the Extracted column's type.\n",
    "        original_casted = col(c).cast(target_type)\n",
    "        extracted_casted = extracted_col.cast(target_type)\n",
    "        \n",
    "        # D. Apply Coalesce\n",
    "        #    coalesce(A, B) returns the first non-null value.\n",
    "        #    Result: Original Value (if exists) OR Extracted Value (if Original was null).\n",
    "        merged_col = coalesce(original_casted, extracted_casted).alias(c)\n",
    "        \n",
    "        final_columns.append(merged_col)\n",
    "        \n",
    "    else:\n",
    "        # --- PASS-THROUGH LOGIC ----------------------------------------------\n",
    "        # For columns like 'id', 'description', 'posting_date', just keep them as is.\n",
    "        final_columns.append(col(c))\n",
    "\n",
    "# 4. EXECUTE MERGE\n",
    "# Apply the projection. This drops 'extracted_data' and updates the target columns.\n",
    "df_final = df_processed.select(*final_columns)\n",
    "\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf22eda3-89bc-4ddf-a9a6-c6ff3c1ad431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.write.mode(\"overwrite\").saveAsTable(\"workspace.car_sales.vehicles_enriched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56491689-197f-4d4a-8ee7-f68ac8ddb4a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_read = spark.table(\"workspace.car_sales.vehicles_enriched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d1daa0-247b-42f8-ae0f-7fe8c46a5bc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf = df_final_read.toPandas()\n",
    "final_null_entries = pdf.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d0534f-dfa1-4704-99e6-46001726467d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of null entries in the initial dataset: {initial_null_entries}\")\n",
    "print(f\"Number of null entries in the final dataset: {final_null_entries}\")\n",
    "print(f\"Recovered entries: {initial_null_entries - final_null_entries}\")\n",
    "print(f\"Percentage of recovered entries: {round((initial_null_entries - final_null_entries) / initial_null_entries * 100, 2)}%\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_enrich_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
